# WGAN-GP (Wasserstein Generative Adversarial Network with Gradient Penalty)

## Table of Contents

1. [Introduction](#introduction)
2. [Background](#background)
3. [Implementation](#implementation)
6. [References](#references)

## 1. Introduction

This README provides a detailed overview of the implementation of the **Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP)** applied to the **celebA** dataset using the **PyTorch** framework. The WGAN-GP is a variant of the original Generative Adversarial Network (GAN) that employs the Wasserstein distance to stabilize the training process and mitigate mode collapse. The gradient penalty further enhances the training stability and helps alleviate the vanishing gradient problem.

## 2. Background

### 2.1 Generative Adversarial Networks (GANs)

Generative Adversarial Networks (GANs) are a class of deep learning models used for generating synthetic data that resembles real data. A GAN consists of two main components: a generator and a discriminator. The generator creates fake data samples, while the discriminator evaluates whether a given sample is real (from the training data) or fake (generated by the generator). The two components are trained in tandem through a minimax game until convergence.

### 2.2 Wasserstein GAN with Gradient Penalty (WGAN-GP)

The Wasserstein GAN (WGAN) introduces the Wasserstein distance metric, also known as the Earth-Mover distance, for measuring the discrepancy between the real and generated data distributions. The WGAN framework improves training stability compared to traditional GANs by avoiding mode collapse and enabling more effective training dynamics.

The Gradient Penalty (GP) is an enhancement to the WGAN that enforces the Lipschitz continuity of the discriminator. This is achieved by penalizing the discriminator's gradient norm with respect to the data samples. The GP regularization term is added to the loss function to ensure that the discriminator's gradients remain within a certain range.

## 3. Implementation

### 3.1 Data Preprocessing

Before training, the celebA dataset is preprocessed to ensure consistent input to the model. Typical preprocessing steps may include resizing images, normalizing pixel values, and data augmentation.

### 3.2 Generator and Discriminator Architectures

The generator and discriminator architectures are defined as neural networks. The generator transforms random noise into synthetic images, while the discriminator evaluates the authenticity of both real and generated images.

### 3.3 Training Procedure

1. **Initialization**: Initialize generator and discriminator weights.

2. **Training Loop**:
   - Sample a batch of real images from the dataset.
   - Generate a batch of fake images using the generator.
   - Compute the Wasserstein distance by optimizing the Wasserstein loss.
   - Apply gradient penalty by optimizing the GP regularization term.
   - Update the discriminator and generator alternatively.

3. **Convergence**:
   - Train until the model converges, indicated by a stable Wasserstein distance and high-quality generated images.

### 3.4 Hyperparameters

Key hyperparameters include learning rates, batch size, number of training iterations, weight clipping (for WGAN), and gradient penalty coefficient (for WGAN-GP).

## 4. References
Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., & Courville, A. C. (2017). Improved training of Wasserstein GANs. In NeurIPS.
  [Link to the paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf)